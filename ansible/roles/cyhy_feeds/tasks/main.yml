---
# tasks file for cyhy_feeds

- name: Create cyhy-feeds config
  copy:
    content: "{{ config }}"
    dest: "/var/cyhy/scripts/cyhy-feeds/cyhy-data-extract.cfg"
    mode: 0444
    owner: cyhy
    group: cyhy

#
# Create the AWS config
#
- name: Create the AWS config
  copy:
    dest: /var/cyhy/scripts/cyhy-feeds/aws_config
    owner: cyhy
    group: cyhy
    mode: 0440
    content: |
      [default]
      region = {{ aws_region }}

      [profile elasticsearch]
      credential_source = Ec2InstanceMetadata
      region = {{ dmarc_import_aws_region }}
      role_arn = {{ dmarc_import_es_role }}

#
# Create yml files for db access
#

# reporter
- name: Create cyhy.yml to store reporter db credentials
  copy:
    dest: /var/cyhy/scripts/cyhy-feeds/cyhy.yml
    owner: cyhy
    group: cyhy
    mode: 0660
    content: |
      database:
        uri: mongodb://{{ reporter_user }}:{{ reporter_pw }}@database1.cyhy:27017/{{ reporter_db }}
        name: {{ reporter_db }}

# scan-reader
- name: Create scan_reader.yml to store scan reader db credentials
  copy:
    dest: /var/cyhy/scripts/cyhy-feeds/scan_reader.yml
    owner: cyhy
    group: cyhy
    mode: 0660
    content: |
      database:
        uri: mongodb://{{ scan_reader_user }}:{{ scan_reader_pw }}@database1.cyhy:27017/{{ scan_reader_db }}
        name: {{ scan_reader_db }}

# assessment-reader
- name: Create assessment_reader.yml to store assessment reader db credentials
  copy:
    dest: /var/cyhy/scripts/cyhy-feeds/assessment_reader.yml
    owner: cyhy
    group: cyhy
    mode: 0660
    content: |
      database:
        uri: mongodb://{{ assessment_read_user }}:{{ assessment_read_pw }}@database1.cyhy:27017/{{ assessment_read_db }}
        name: {{ assessment_read_db }}

#
# Import keys and trust
#

# The --batch flag makes sure that gpg2 doesn't attempt to do anything
# interactive.
- name: Import gpg keys
  shell: "echo '{{ item }}' | gpg2 --trustdb-name /var/cyhy/.gnupg/trustdb.gpg --import --batch"
  become_user: cyhy
  vars:
    ansible_ssh_pipelining: yes
  loop:
    - "{{ ncps_ae_public_gpg_key }}"
    - "{{ nsd_public_gpg_key }}"
    - "{{ private_gpg_key }}"
    - "{{ public_gpg_key }}"
  loop_control:
    label: "<key redacted>"

- name: Import gpg trust
  shell: "echo {{ gpg_trust }} | gpg2 --import-ownertrust --batch"
  become_user: cyhy
  vars:
    ansible_ssh_pipelining: yes

#
# Create a cron job to run the extract script nightly at 0000 (UTC) as cyhy
# NOTE:
# We run at this time to reduce the odds of having documents, specifically
# tickets, get missed in successive daily extracts. The previous time of 08:15
# left an eight hour gap between the close of the query window for the script
# and the start of the extract process. Since we are continuously scanning this
# resulted in documents being updated and their modification time being updated
# to fall past the end of the query cutoff. Changes were made to the script in
# relation to this issue, and more information can be found in the pull request at
# https://github.com/cisagov/cyhy-feeds/pull/37
#
- name: Set up nightly cron job to sync NSD data for MOE extract
  cron:
    name: Nightly cyhy extract
    hour: '0'
    minute: '0'
    user: cyhy
    job: cd /var/cyhy/scripts/cyhy-feeds && export AWS_CONFIG_FILE=/var/cyhy/scripts/cyhy-feeds/aws_config; python2.7 /var/cyhy/scripts/cyhy-feeds/cyhy-data-extract.py --cyhy-config /var/cyhy/scripts/cyhy-feeds/cyhy.yml --scan-config /var/cyhy/scripts/cyhy-feeds/scan_reader.yml --assessment-config /var/cyhy/scripts/cyhy-feeds/assessment_reader.yml --aws --cleanup-aws --config /var/cyhy/scripts/cyhy-feeds/cyhy-data-extract.cfg 2>&1 | /usr/bin/logger -t cyhy-feeds
  when: production_workspace|bool
